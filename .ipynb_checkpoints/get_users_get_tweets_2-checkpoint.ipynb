{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established Twitter connection.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import ConfigParser\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import sys\n",
    "import time\n",
    "from TwitterAPI import TwitterAPI\n",
    "import pickle\n",
    "import os.path\n",
    "import re \n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import codecs\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def get_twitter(config_file):\n",
    "    \"\"\" Read the config_file and construct an instance of TwitterAPI.\n",
    "    Args:\n",
    "      config_file ... A config file in ConfigParser format with Twitter credentials\n",
    "    Returns:\n",
    "      An instance of TwitterAPI.\n",
    "    \"\"\"\n",
    "    config = ConfigParser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    twitter = TwitterAPI(\n",
    "                   config.get('twitter', 'consumer_key'),\n",
    "                   config.get('twitter', 'consumer_secret'),\n",
    "                   config.get('twitter', 'access_token'),\n",
    "                   config.get('twitter', 'access_token_secret'))\n",
    "    return twitter\n",
    "\n",
    "twitter = get_twitter('twitter.cfg')\n",
    "print('Established Twitter connection.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def robust_request(twitter, resource, params, max_tries=5):\n",
    "    \"\"\" If a Twitter request fails, sleep for 15 minutes.\n",
    "    Do this at most max_tries times before quitting.\n",
    "    Args:\n",
    "      twitter .... A TwitterAPI object.\n",
    "      resource ... A resource string to request.\n",
    "      params ..... A parameter dictionary for the request.\n",
    "      max_tries .. The maximum number of tries to attempt.\n",
    "    Returns:\n",
    "      A TwitterResponse object, or None if failed.\n",
    "    \"\"\"\n",
    "    for i in range(max_tries):\n",
    "        request = twitter.request(resource, params)\n",
    "        if request is None:\n",
    "            print ('Found None object')\n",
    "            break\n",
    "        if request.status_code == 401:\n",
    "            print ('Unauthorized data access. The user does not have access to such information')\n",
    "            break\n",
    "#         elif \"page does not exist\" in request.text:\n",
    "#             print 'page does not exist'\n",
    "#             break\n",
    "        elif request.status_code == 200:\n",
    "            return request\n",
    "        else:\n",
    "            print >> sys.stderr, 'Got error:', request.text, '\\nsleeping for 15 minutes.'\n",
    "            sys.stderr.flush()\n",
    "            time.sleep(61 * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6533\n"
     ]
    }
   ],
   "source": [
    "def get_followers():\n",
    "    followers = []\n",
    "    f = open ('iit_followers.txt','r')\n",
    "    regexp = re.compile(r'p\\d+')\n",
    "    for line in f:\n",
    "        #print(str(line))\n",
    "        if regexp.search(str(line)) is not None:\n",
    "            continue\n",
    "        else: \n",
    "            if (str(line)).startswith('aV'):\n",
    "                line = line[2:]\n",
    "                followers.append(line.replace(\"\\n\",\"\"))\n",
    "    followers.sort()\n",
    "    return followers\n",
    "\n",
    "followers_list = get_followers()\n",
    "print (len(followers_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "processed 100 users...\n",
      "done\n",
      "6471\n"
     ]
    }
   ],
   "source": [
    "#get first 100 user IDs\n",
    "userIds = []\n",
    "startIndex = 0\n",
    "endIndex = 100\n",
    "while endIndex < len(followers_list):\n",
    "    userInfo = twitter.request('users/lookup', {'screen_name': followers_list[startIndex:endIndex]})\n",
    "    for user in userInfo:\n",
    "        userIds.append(user['id'])\n",
    "    startIndex += 100\n",
    "    endIndex += 100\n",
    "    print ('processed 100 users...')\n",
    "print ('done')\n",
    "print (len(userIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2885816063\n"
     ]
    }
   ],
   "source": [
    "userInfo = twitter.request('users/lookup', {'screen_name': 'hpandya2_pa'})\n",
    "for user in userInfo:\n",
    "    userIds.insert(0, user['id'])\n",
    "    print (user['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_users():\n",
    "    print ('in get_users()')\n",
    "    request = twitter.request('statuses/filter', {'follow':userIds[:5000], 'filter_level':'low'})\n",
    "    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    newfile = 'streaming_user_tweets_' + timestamp + '.bin'\n",
    "    numTweets = 0\n",
    "    print ('starting collection!!! :)')\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            for tweet in request:\n",
    "                print (\"got a tweet\")\n",
    "                try:\n",
    "                    print (tweet['text'])\n",
    "                    print ('')\n",
    "                except:\n",
    "                    continue\n",
    "                pickle.dump(tweet, open(newfile, 'ab'))\n",
    "                numTweets += 1\n",
    "                print ('dumped tweet to file')\n",
    "                if numTweets > 5000:\n",
    "                    numTweets = 1\n",
    "                    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    newfile = 'streaming_user_tweets_' + timestamp + '.bin' \n",
    "        except:\n",
    "            print ('Got error: (sleeping for 60 seconds...)')\n",
    "            print (request.text)\n",
    "            sys.stderr.flush()\n",
    "            time.sleep(60) \n",
    "               \n",
    "#get_users()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def readFileAndLabelTweets(skipFirstNTweets,tweetFiles):\n",
    "    #home_path=(os.path.expanduser('~/Desktop/CS579_Project/CS579_Project'))\n",
    "    #tweetFiles = 'convertedIITTweets.bin'\n",
    "    tweetNumber = 1\n",
    "    tweets = []\n",
    "    \n",
    "    openedFile = open(tweetFiles, 'rb')\n",
    "    while True:   \n",
    "        try:    \n",
    "            loadedTweet = pickle.load(openedFile)\n",
    "                \n",
    "            if skipFirstNTweets + 1 > tweetNumber:\n",
    "                tweetNumber+= 1\n",
    "                continue\n",
    "\n",
    "            if 'text' in loadedTweet:\n",
    "                thisTweet = loadedTweet['text']\n",
    "                #only look at unique tweets during classification\n",
    "                if thisTweet in tweets:\n",
    "                    continue\n",
    "                else:\n",
    "                    tweets.append(thisTweet)\n",
    "\n",
    "                print (\"%d) %s\\n\" % (tweetNumber, thisTweet))\n",
    "                #classify...\n",
    "                label = input('Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): ')\n",
    "                if label == 1:\n",
    "                    print('academics\\n')\n",
    "                elif label == 2:\n",
    "                    print('sports\\n')\n",
    "                elif label == 3:\n",
    "                    print('politics\\n')\n",
    "                elif label == 4:\n",
    "                    print('technology\\n')\n",
    "                elif label == 5:\n",
    "                    print('current_affairs\\n')\n",
    "                elif label == 6:\n",
    "                    print ('irrelevant')\n",
    "                elif label == 7:\n",
    "                    print ('campus')\n",
    "                elif label == 8:\n",
    "                    print ('stopping now')\n",
    "                    print('labeled %d tweets, ending with tweet #%d' % ((len(tweets) - 1),(tweetNumber - 1)))\n",
    "                    return\n",
    "\n",
    "                #save tweet text with a label\n",
    "                tweetTextWithLabel = {'label': str(label), 'tweetText': loadedTweet['text']}\n",
    "                fileName = 'labelsFor_' + tweetFiles\n",
    "                pickle.dump(tweetTextWithLabel, open(fileName, 'ab'))\n",
    "\n",
    "                tweetNumber += 1\n",
    "            else:\n",
    "                continue\n",
    "        except EOFError:\n",
    "            print (\"end of file...exiting\")\n",
    "            break\n",
    "\n",
    "#allow to resume adding labels from the middle of tweets\n",
    "skipFirstNTweets = 0\n",
    "#readFileAndLabelTweets(skipFirstNTweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readFileAndLabelTweets(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(259)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " readFileAndLabelTweets(519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(553)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readFileAndLabelTweets(682)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(685)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(0,'convertedNUTweets.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(0,'pickle_nu_outputfile_2015-11-26 12:58:06.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " readFileAndLabelTweets(63,'pickle_nu_outputfile_2015-11-26 12:58:06.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of file...exiting\n"
     ]
    }
   ],
   "source": [
    "readFileAndLabelTweets(86,'pickle_nu_outputfile_2015-11-26 12:58:06.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(0,'pickle_nu_outputfile_2015-11-26 13:04:29.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### readFileAndLabelTweets(0,'pickle_nu_outputfile_2015-11-26 14:28:24.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(11,'pickle_nu_outputfile_2015-11-26 14:28:24.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(85,'pickle_nu_outputfile_2015-11-26 14:28:24.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(86,'pickle_nu_outputfile_2015-11-26 14:28:24.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(90,'pickle_nu_outputfile_2015-11-26 14:28:24.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(226,'pickle_nu_outputfile_2015-11-26 14:28:24.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(401,'pickle_nu_outputfile_2015-11-26 14:28:24.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) RT @pickover: Sometimes the beauty of physics and optics is a form of poetry, or of love. Sunset curling up along a wave. http://t.co/sttSW‚Ä¶\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 1\n",
      "academics\n",
      "\n",
      "2) RT @IndianExpress: Nagaur Fair: Glimpse into one of the largest cattle fair in Rajasthan shot by @achaoklater http://t.co/1XeTvuZOLm http:/‚Ä¶\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "3) RT @MaheshNBhatt: Pause and read: http://t.co/iPfkpx0ktS\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "4) http://t.co/pd63Yl8nBX\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "5) http://t.co/l06VzNEgG8\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "6) \"@FindTheWild: Grand Canyon of Yellowstone http://t.co/nNBk6QT3i5\"\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "7) RT @Discovery: Are polar bears saving themselves? http://t.co/ccCx8KT22n http://t.co/qnmb4qHBm9\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "8) @Discovery wonderful picture with a message from nature üçÄ\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "9) 2015 violence on the roll...waiting for humanity to rise above all odds...and keeps on rising üçÄ\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "10) #GoT #S04E09 surely reminded me of the movie Kingdom of Heaven\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "11) @_Sibaprasad_ I can see why are you shifting your attention from roorkee to Delhi now a days :-P\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "12) Greatful to spend the day with mother on this special day @Techfest_IITB #MothersDay\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "13) @_Sibaprasad_ well, is it the main gate which decides the reputation of an institute then?\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "14) Well, you don't witness such great chase usually #RRvsRCB\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "15) I'm Mycroft Holmes! #ZimbioQuiz http://t.co/Oo51opQX6b\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "16) I turned my computer into a Wi-Fi Hotspot using #Connectify. Share Internet with the awesome @connectifyme app here:\n",
      " http://t.co/f9bmTW2nvJ\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "17) ok.. so #Best trying to impress God by the bouncers? No way he can get that :D #ThankYouSachin\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "18) unpredictable and never easy #Life\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "19) there are many #FriendsAndFashion, but dont make smoking one of them #ISMOKE\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "20) #ISMOKE Because my girlfriend has everything to kill me ;)\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "21) Happy b'day @saqib0308 Great day to quit smoking #ISMOKE\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): 6\n",
      "irrelevant\n",
      "22) #WeAllHaveThatOneFriendWho smokes, let him help quit smoking #ISMOKE\n",
      "\n",
      "Tweet label (1=academics, 2=sports, 3=politics, 4=technology, 5=current_affairs, 6=irrevelant, 7=campus, 8=stop): \n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<string>, line 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "readFileAndLabelTweets(0,'pickle_outputfile_2015-11-17 18:32:38.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(22,'pickle_outputfile_2015-11-17 18:32:38.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(233,'pickle_outputfile_2015-11-17 18:32:38.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(0,'pickle_outputfile_2015-11-17 20:57:07.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### readFileAndLabelTweets(162,'pickle_outputfile_2015-11-17 20:57:07.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(162,'pickle_outputfile_2015-11-17 20:57:07.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#readFileAndLabelTweets(243,'pickle_outputfile_2015-11-17 20:57:07.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweetFiles = 'convertedIITTweets.bin'\n",
    "# for f in tweetFiles:\n",
    "#     openedFile = open(f, 'rb')\n",
    "#     loadedTweet = pickle.load(openedFile)\n",
    "#     print loadedTweet\n",
    "#     break\n",
    "\n",
    "import os.path\n",
    "home_path=(os.path.expanduser('~/Desktop/CS579_Project/CS579_Project/'))\n",
    "# if os.path.isfile('streaming_user_tweets_2015-11-21 01:20:40.bin'):\n",
    "#     print 'true'\n",
    "# else:\n",
    "#     print 'false'\n",
    "    \n",
    "# for f in tweetFiles:\n",
    "#     openedFile = open(join(home_path,f), 'rb')\n",
    "#     loadedTweet = pickle.load(openedFile)\n",
    "#     print loadedTweet\n",
    "#     break  \n",
    "\n",
    "# files = listdir(home_path)\n",
    "# for f in files:\n",
    "#     if f.endswith('.bin'):\n",
    "#         print f\n",
    "# openedFile = open(tweetFiles, 'rb')\n",
    "# loadedTweet = pickle.load(openedFile)\n",
    "# print loadedTweet\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def readTweetTextWithLabel(filename):\n",
    "    #tweetFiles = ['labelsFor_convertedIITTweets.bin']\n",
    "    tweetFiles = [filename]\n",
    "    tweetNumber = 1\n",
    "    tweets = []\n",
    "    tweetTextWithLabel = []\n",
    "    for file in tweetFiles:\n",
    "        print ('getting tweets with label from file', file)\n",
    "        openedFile = open(file, 'rb')\n",
    "        while True:   \n",
    "            try:    \n",
    "                loadedTweet = pickle.load(openedFile)\n",
    "                \n",
    "                #this will only run if multiple labels given to same tweet\n",
    "                if 'tweetText' in loadedTweet:\n",
    "                    thisTweetText = loadedTweet['tweetText']\n",
    "                    #only look at unique tweets during classification\n",
    "                    if thisTweetText in tweets:\n",
    "                        continue\n",
    "                    else:\n",
    "                        tweets.append(thisTweetText)\n",
    "                        tweetTextWithLabel.append(loadedTweet)\n",
    "                    labelText = ''\n",
    "                    if loadedTweet['label'] == '1':\n",
    "                        labelText = '[ACADEMICS]'\n",
    "                    elif loadedTweet['label'] == '2':\n",
    "                        labelText = '[SPORTS]'\n",
    "                    elif loadedTweet['label'] == '3':\n",
    "                        labelText = '[POLITICS]'\n",
    "                    elif loadedTweet['label'] == '4':\n",
    "                        labelText = '[TECHNOLOGY]'\n",
    "                    elif loadedTweet['label'] == '5':\n",
    "                        labelText = '[CURRENT_AFFAIRS]'\n",
    "                    elif loadedTweet['label'] == '6':\n",
    "                        labelText = '[IRRELEVANT]'\n",
    "                    elif loadedTweet['label'] == '7':\n",
    "                        labelText = '[CAMPUS]'\n",
    "                    print (\"%d) %-6s %s\\n\" % (tweetNumber, labelText, thisTweetText))\n",
    "                    tweetNumber += 1\n",
    "                else:\n",
    "                    continue\n",
    "            except EOFError:\n",
    "                print (\"end of file...exiting\")\n",
    "                break\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#readTweetTextWithLabel('labelsFor_pickle_nu_outputfile_2015-11-26 12:58:06.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#readTweetTextWithLabel('labelsFor_pickle_nu_outputfile_2015-11-26 13:04:29.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#readTweetTextWithLabel('labelsFor_pickle_nu_outputfile_2015-11-26 14:28:24.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#readTweetTextWithLabel('labelsFor_pickle_outputfile_2015-11-17 20:57:07.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#readTweetTextWithLabel('labelsFor_pickle_outputfile_2015-11-17 18:32:38.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('getting tweets with label from file', 'labelsFor_convertedNUTweets.bin')\n",
      "1) [IRRELEVANT] RT @GE_Foundation: How #apprenticeships can help solve the #skillsgap: https://t.co/dCoSGS42Dz\n",
      "\n",
      "2) [CURRENT_AFFAIRS] RT @Ostrov_A: Wishing all #ShabbatShalom, especially in #Israel, where we pray it is a peaceful #Shabbat.\n",
      "\n",
      "3) [IRRELEVANT] Find out more about @IL_Hunger and they work the do in #Chicago in the #StreetWise #Nonprofit guide! https://t.co/aMIDdSDZb9\n",
      "\n",
      "4) [IRRELEVANT] RT @ColeyHarvey: Think the Bengals are tight? Before Friday's walkthrough, righty Andy Dalton throws... https://t.co/kq769zzpUy https://t.c‚Ä¶\n",
      "\n",
      "5) [IRRELEVANT] RT @Ostrov_A: #BDSFail of week! Pro-#BDS hate group 'Students for Justice in #Palestine' uses #Israeli co. @Wix to build website https://t.‚Ä¶\n",
      "\n",
      "6) [IRRELEVANT] https://t.co/Wwx5SV1JfY\n",
      "\n",
      "#EURUSD  &lt; 1.06 pour finir la semaine\n",
      "en attendant #BCE &amp; #FED en Decembre https://t.co/XVdk5DpwQi\n",
      "\n",
      "7) [IRRELEVANT] RT @BBGMedia: How Amazon makes Black Friday happen https://t.co/EaeCgTRif7 https://t.co/WKJu7iGuWK\n",
      "\n",
      "8) [IRRELEVANT] RT @DailyMailCeleb: Tom Hanks opens up about his wife's battle with cancer https://t.co/BCpZa9Dhl7 https://t.co/qfKapknXRJ\n",
      "\n",
      "9) [IRRELEVANT] RT @Ostrov_A: She's been a good girl all day. Yeah, I think she deserves trip to the #dog park! #woofwoof https://t.co/lj37g2BGMN\n",
      "\n",
      "10) [IRRELEVANT] RT @LekeishaSumner: Interesting take on academia &amp; Kim Kardashian. Why we all need to keep up with the Kardashians https://t.co/4nF511m5ui ‚Ä¶\n",
      "\n",
      "11) [IRRELEVANT] RT @ElsevierConnect: What patients &amp; physicians need to know for #prostate cancer screenings. @PennMedicine expert video Q&amp;A.  #Movember ht‚Ä¶\n",
      "\n",
      "12) [IRRELEVANT] @AmigosInDanger GOOD LUCK ALLIE ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏èüíûüíû\n",
      "\n",
      "13) [IRRELEVANT] TODAY ONLY: Use promo code BLACKFRIDAY here: https://t.co/MGLg5E1t7l to save $5 on Beer Fest Beatdown tickets! https://t.co/2LVSoq8Pru\n",
      "\n",
      "14) [IRRELEVANT] RT @LoriMoreno: #Love #Romance #Nature #Fitness #Serenity \n",
      "#Luxury #Travel #Hikes #travelfoodiesTV\n",
      "https://t.co/QK4AzthBDq @GoPro https://t‚Ä¶\n",
      "\n",
      "15) [IRRELEVANT] RT @LoriMoreno: Which 5 cities do you want to #travel to? #adventure #tours #luxury #luxurytravel #lovetravel #glamping #wishlist https://t‚Ä¶\n",
      "\n",
      "16) [IRRELEVANT] @imagine_samar WOW! Samar, this is awesome!\n",
      "\n",
      "17) [IRRELEVANT] Unplag provides top solutions to #education #highered #edtech #edtechchat Stop by our Stand #C104 in @OEBconference #OEB15 #fightplagiarism\n",
      "\n",
      "18) [IRRELEVANT] RT @alightlover: Here's hoping there are some Black Friday deals on students loans, am I right?!\n",
      "\n",
      "19) [IRRELEVANT] Sure-Fit Deluxe Comfort Club Chair Slipcover https://t.co/w8hAFC9W8S https://t.co/PosuRHtzjZ\n",
      "\n",
      "20) [IRRELEVANT] Sure-Fit Deluxe Comfort Club Chair Slipcover https://t.co/w8hAFC9W8S https://t.co/cDXE1bFXVX\n",
      "\n",
      "21) [IRRELEVANT] Daphne Sky Blue Studded Zircon Pendant and Earrings for Anniversary Gift https://t.co/1OdZRsuZwN https://t.co/HSzvbcYYsj\n",
      "\n",
      "22) [IRRELEVANT] RT @SPSCC: REMINDER: We are closed today in observance of the Thanksgiving holiday.\n",
      "\n",
      "23) [IRRELEVANT] \"8.92 CT\" DAZZLING RARE HUGE NATURAL EARTHMINED HESSONITE GARNET FROM SRILANKA https://t.co/w7M0Ip43Dk https://t.co/96W2RqLOhB\n",
      "\n",
      "24) [IRRELEVANT] RT @amyjccuddy: I cannot wait until Feb 3 to read the new book by @AdamMGrant - Originals. https://t.co/LRU75Kev9C\n",
      "\n",
      "25) [IRRELEVANT] Creative Circle FESTIVE DELIGHTS Christmas Crewel Embroidery KIT #2253 4 Designs https://t.co/LtLavE5aHB https://t.co/Y676jJ1oKe\n",
      "\n",
      "26) [IRRELEVANT] Canada Scotts # 2253 used,, https://t.co/hktuva94hN https://t.co/3FHlkOVdT8\n",
      "\n",
      "27) [IRRELEVANT] Canada Scotts # 2253B used,, https://t.co/qdrq4vLSHM https://t.co/JjeAi6ZqG2\n",
      "\n",
      "28) [CURRENT_AFFAIRS] RT @fox32news: \"They are going to be obliterated. That gang just signed its death warrant\" - Garry McCarthy https://t.co/HVa3Z7Z16P\n",
      "\n",
      "29) [IRRELEVANT] RT @DNAinfoCHI: \"We are going to assign resources to make sure that neither one of those gangs will raise its head again.\" - Supt. McCarthy‚Ä¶\n",
      "\n",
      "30) [CURRENT_AFFAIRS] RT @craigrwall: McCarthy: the gang responsible for Tyshawn Lee's murder \"just signed its own death warrant\" and will be \"obliterated.\" @fox‚Ä¶\n",
      "\n",
      "31) [IRRELEVANT] #travel @FourSeasons 8 iconic destinations. One elegant trip journey of a lifetime: https://t.co/i5QuY9tJXu  #FSJet https://t.co/pVtwpg4pzq\n",
      "\n",
      "32) [IRRELEVANT] Forex\n",
      "\n",
      " https://t.co/YX4TSY3Vo4\n",
      "\n",
      "#EURUSD  -  #USDJPY \n",
      "\n",
      "^^^\"\"\" https://t.co/UeuPfWF0wU\n",
      "\n",
      "33) [IRRELEVANT] RT @CurlsandCompany: Curly turkey day Queen!\n",
      "\n",
      "end of file...exiting\n"
     ]
    }
   ],
   "source": [
    "readTweetTextWithLabel('labelsFor_convertedNUTweets.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def tokenize(text):\n",
    "    \"\"\"Given a string, return a list of tokens such that: (1) all\n",
    "    tokens are lowercase, (2) all punctuation is removed. Note that\n",
    "    underscore (_) is not considered punctuation.\n",
    "    Params:\n",
    "        text....a string\n",
    "    Returns:\n",
    "        a list of tokens\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    ###\n",
    "    result = re.findall('\\w\\w+',text.lower())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of file... exiting\n",
      "number of tweets found=  4734\n",
      "End of creating vocabulary\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def vocabulary():\n",
    "    openedFile = open('convertedIITTweets.bin','rb')\n",
    "    tweets = []\n",
    "    tokens = set()\n",
    "    tweet_count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            loadedTweet = pickle.load(openedFile)\n",
    "            if loadedTweet:\n",
    "                tweets.append(loadedTweet['text'])\n",
    "                #print 'added tweet ', tweet_count\n",
    "                tweet_count += 1\n",
    "        except EOFError:\n",
    "            print 'end of file... exiting'\n",
    "            break\n",
    "    print 'number of tweets found= ', tweet_count\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        tokens.update(tokenize(tweet))\n",
    "    print 'End of creating vocabulary'\n",
    "    return tweets, tokens\n",
    "\n",
    "vocabulary = vocabulary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'tuple'>\n",
      "<type 'set'>\n",
      "raining\n",
      "yoda_bot_tweet\n",
      "culeros\n",
      "four\n",
      "brainturbo\n",
      "woods\n",
      "fearmongers\n",
      "hanging\n",
      "global_entrepreneurship_week\n",
      "thevillageinn\n",
      "chemicalinternships\n"
     ]
    }
   ],
   "source": [
    "print type(vocabulary)    \n",
    "print type(vocabulary[1])\n",
    "count = 0\n",
    "for token in vocabulary[1]:\n",
    "    if count <= 10:\n",
    "        print token\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "def count_vocab():\n",
    "    tweets = vocabulary[0]\n",
    "    vocab_words = vocabulary[1]\n",
    "    count = Counter()\n",
    "    for tweet in tweets:\n",
    "        count.update(tokenize(tweet))\n",
    "        #sorted(count, key=lambda x: , reverse=True))\n",
    "    return count\n",
    "\n",
    "counter = count_vocab()\n",
    "print len(counter.keys())\n",
    "# for key,value in enumerate(counter):\n",
    "#     print key,value\n",
    "#print counter.most_common(10)\n",
    "# counter['school']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n"
     ]
    }
   ],
   "source": [
    "print type(counter)\n",
    "# print help(counter)\n",
    "#print counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
