{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import ConfigParser\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import sys\n",
    "import time\n",
    "from TwitterAPI import TwitterAPI\n",
    "import pickle\n",
    "import os.path\n",
    "import re \n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import codecs\n",
    "from os.path import isfile, join\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established Twitter connection.\n"
     ]
    }
   ],
   "source": [
    "def get_twitter(config_file):\n",
    "    \"\"\" Read the config_file and construct an instance of TwitterAPI.\n",
    "    Args:\n",
    "      config_file ... A config file in ConfigParser format with Twitter credentials\n",
    "    Returns:\n",
    "      An instance of TwitterAPI.\n",
    "    \"\"\"\n",
    "    config = ConfigParser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    twitter = TwitterAPI(\n",
    "                   config.get('twitter', 'consumer_key'),\n",
    "                   config.get('twitter', 'consumer_secret'),\n",
    "                   config.get('twitter', 'access_token'),\n",
    "                   config.get('twitter', 'access_token_secret'))\n",
    "    return twitter\n",
    "\n",
    "twitter = get_twitter('twitter.cfg')\n",
    "print('Established Twitter connection.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def robust_request(twitter, resource, params, max_tries=5):\n",
    "    \"\"\" If a Twitter request fails, sleep for 15 minutes.\n",
    "    Do this at most max_tries times before quitting.\n",
    "    Args:\n",
    "      twitter .... A TwitterAPI object.\n",
    "      resource ... A resource string to request.\n",
    "      params ..... A parameter dictionary for the request.\n",
    "      max_tries .. The maximum number of tries to attempt.\n",
    "    Returns:\n",
    "      A TwitterResponse object, or None if failed.\n",
    "    \"\"\"\n",
    "    for i in range(max_tries):\n",
    "        request = twitter.request(resource, params)\n",
    "        if request is None:\n",
    "            print ('Found None object')\n",
    "            break\n",
    "        if request.status_code == 401:\n",
    "            print ('Unauthorized data access. The user does not have access to such information')\n",
    "            break\n",
    "#         elif \"page does not exist\" in request.text:\n",
    "#             print 'page does not exist'\n",
    "#             break\n",
    "        elif request.status_code == 200:\n",
    "            return request\n",
    "        else:\n",
    "            print >> sys.stderr, 'Got error:', request.text, '\\nsleeping for 1 minutes.'\n",
    "            sys.stderr.flush()\n",
    "            time.sleep(61 * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This method collects the usernames of university in a file named filename\n",
    "def unpickle_usernames(filename):\n",
    "    followers = []\n",
    "    f = open(filename,'rb')\n",
    "    followers = pickle.load(f)\n",
    "    \n",
    "    print 'End of collecting usernames from the file' , filename\n",
    "    return followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of collecting usernames from the file iit_followers.txt\n",
      "End of collecting usernames from the file nu_followers.txt\n",
      "no_of_iit_users 6565\n",
      "no_of_nu_users 35984\n"
     ]
    }
   ],
   "source": [
    "iit_users = unpickle_usernames('iit_followers.txt')\n",
    "nu_users = unpickle_usernames('nu_followers.txt')\n",
    "print 'no_of_iit_users' , len(iit_users)\n",
    "print 'no_of_nu_users' , len(nu_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''This method collects tweets of all the followers of IIT's different twitter profiles and pickles them to a file\n",
    "    Each file is time stamped and creates a new file at every 5000 tweets\n",
    "    It takes the name of the output file and index in followers_list (list of screen_names of iit followers) \n",
    "    where one would like to start data collection. \n",
    "''' \n",
    "def collect_tweets(outputfile,follower_index):\n",
    "    tweets = 0\n",
    "    followers = followers_list[follower_index:]\n",
    "    f = codecs.open(outputfile,'a',encoding='utf8')\n",
    "    for follower in followers:\n",
    "        print \"processing follower %s with index %d in followers_list\" %(follower, followers_list.index(follower))\n",
    "        request = robust_request(twitter,'statuses/user_timeline',{'screen_name': follower})\n",
    "        if request is not None:\n",
    "            if tweets <= 10000:\n",
    "                for tweet in request:\n",
    "                    if tweet is not None:\n",
    "                        tweets += 1\n",
    "                        json.dump(tweet,f)\n",
    "                        f.write('\\n')\n",
    "                        print 'dumped tweet no %d' %(tweets)\n",
    "            else:\n",
    "                timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                newfile = 'nu_outputfile_' + timestamp + '.txt'\n",
    "                print 'calling new file name', newfile\n",
    "                print_tweets(newfile,followers_list.index(follower))\n",
    "    print 'End of collecting tweets of followers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''This method is a helper function for labeling process.\n",
    "   It reads the saved tweets from a file and prompts for a label \n",
    "   and saves the tweet and its corresponding label in a different file\n",
    "'''\n",
    "def read_file_and_label_tweets(skipFirstNTweets,tweetFiles):\n",
    "    \n",
    "    tweetNumber = 1\n",
    "    tweets = []\n",
    "    for file in tweetFiles:\n",
    "        print ('getting tweets from file', file)\n",
    "        openedFile = open(file, 'rb')\n",
    "        while True:   \n",
    "            try:    \n",
    "                loadedTweet = pickle.load(openedFile)\n",
    "                \n",
    "                if skipFirstNTweets + 1 > tweetNumber:\n",
    "                    tweetNumber+= 1\n",
    "                    continue\n",
    "                \n",
    "                if 'text' in loadedTweet:\n",
    "                    thisTweet = loadedTweet['text']\n",
    "                    #only look at unique tweets during classification\n",
    "                    if thisTweet in tweets:\n",
    "                        continue\n",
    "                    else:\n",
    "                        tweets.append(thisTweet)\n",
    "\n",
    "                    print (\"%d) %s\\n\" % (tweetNumber, thisTweet))\n",
    "                    #classify...\n",
    "                    label = input('Tweet label (1=good, 2=bad, 3=irrelevant, 4=stop): ')\n",
    "                    if label == '1':\n",
    "                        print('good\\n')\n",
    "                    elif label == '2':\n",
    "                        print('bad\\n')\n",
    "                    elif label == '3':\n",
    "                        print('irrelevant\\n')\n",
    "                    elif label == '4':\n",
    "                        print('stopping\\n')\n",
    "                        print('labeled %d tweets, ending with tweet #%d' % ((len(tweets) - 1),(tweetNumber - 1)))\n",
    "                        return\n",
    "                    \n",
    "                    #save tweet text with a label\n",
    "                    tweetTextWithLabel = {'label': label, 'tweetText': loadedTweet['text']}\n",
    "                    fileName = 'labelsFor' + file\n",
    "                    pickle.dump(tweetTextWithLabel, open(fileName, 'ab'))\n",
    "                    \n",
    "                    tweetNumber += 1\n",
    "                else:\n",
    "                    continue\n",
    "            except EOFError:\n",
    "                print (\"end of file...exiting\")\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' This method takes a list of filenames and loads the labeled tweets\n",
    "    in a list.'''\n",
    "def read_pickled_tweets(filenames):\n",
    "    tweets = []\n",
    "    for filename in filenames:\n",
    "        f = open(filename,'rb')\n",
    "        while True:\n",
    "            try:\n",
    "                loadedtweet = pickle.load(f)\n",
    "                tweets.append(loadedtweet)\n",
    "            except EOFError:\n",
    "                print 'end of file... exiting'\n",
    "                break\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def create_X_and_Y_vectors(labeledTweets):\n",
    "#     vectorizer = CountVectorizer(min_df=2)\n",
    "#     X = vectorizer.fit_transform(tweet['tweetText'] for tweet in labeledTweets)\n",
    "#     y = numpy.array([int(tweet['label']) for tweet in labeledTweets])\n",
    "#     return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_cross_validation(X,y,n):\n",
    "    accuracyPerFold = []\n",
    "    for trainingIndex, testingIndex in KFold(len(y), n):\n",
    "        model.fit(X[trainingIndex], y[trainingIndex])\n",
    "        predictions = model.predict(X[testingIndex])\n",
    "        accuracyPerFold.append(accuracy_score(y[testingIndex], predictions))\n",
    "\n",
    "    print 'accuracy per {0}-fold: {1}'.format(n, accuracyPerFold)\n",
    "    print ('average accuracy per fold:', numpy.mean(accuracyPerFold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_school_matrix(list_of_files,max_features):\n",
    "    list_of_tweets = read_pickled_tweets(list_of_files)\n",
    "    vectorizer = CountVectorizer(min_df=2,max_features=max_features)\n",
    "    X = vectorizer.fit_transform(tweet['text'] for tweet in list_of_tweets)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(trueLabel, predictedLabel):\n",
    "    return (1. * len([True for t, p in zip(trueLabel, predictedLabel) if t == p]) / len(trueLabel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percentage(list_of_values):\n",
    "    result = []\n",
    "    total = sum(list_of_values)\n",
    "    result = [(float(v)/total)*100 for v in list_of_values]\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of file... exiting\n",
      "end of file... exiting\n",
      "end of file... exiting\n",
      "end of file... exiting\n",
      "end of file... exiting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1987"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_tweets = read_pickled_tweets(['labelsFor_convertedIITTweets.bin', 'labelsFor_pickle_nu_outputfile_2015-11-26 12:58:06.txt', 'labelsFor_pickle_nu_outputfile_2015-11-26 14:28:24.txt','labelsFor_pickle_outputfile_2015-11-17 18:32:38.txt','labelsFor_pickle_outputfile_2015-11-17 20:57:07.txt'])\n",
    "len(training_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('reduced tweet count:', 1987)\n",
      "('accuracy fitting to all data:', 0.95067941620533469)\n"
     ]
    }
   ],
   "source": [
    "X,y = create_X_and_Y_vectors(training_tweets)\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "print('reduced tweet count:', len(y))\n",
    "print('accuracy fitting to all data:', accuracy_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy per 5-fold: [0.79145728643216084, 0.8266331658291457, 0.78589420654911835, 0.81612090680100757, 0.85642317380352639]\n",
      "('average accuracy per fold:', 0.81530574788299182)\n"
     ]
    }
   ],
   "source": [
    "do_cross_validation(X,y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy per 10-fold: [0.70854271356783916, 0.87939698492462315, 0.89447236180904521, 0.76884422110552764, 0.74371859296482412, 0.83417085427135673, 0.8040201005025126, 0.81818181818181823, 0.8232323232323232, 0.90404040404040409]\n",
      "('average accuracy per fold:', 0.8178620374600275)\n"
     ]
    }
   ],
   "source": [
    "do_cross_validation(X,y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_training_tweets = [tweet for tweet in training_tweets if tweet['label'] != '6']\n",
    "len(filtered_training_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n",
      "(378, 717)\n",
      "('reduced tweet count:', 378)\n",
      "('accuracy fitting to all data:', 0.97089947089947093)\n"
     ]
    }
   ],
   "source": [
    "X_new,y_new = create_X_and_Y_vectors(filtered_training_tweets)\n",
    "model_new = LogisticRegression()\n",
    "model_new.fit(X_new, y_new)\n",
    "predictions_new = model_new.predict(X_new)\n",
    "print len(y_new)\n",
    "print X_new.shape\n",
    "print('reduced tweet count:', len(y_new))\n",
    "print('accuracy fitting to all data:', accuracy_score(y_new, predictions_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n",
      "(378, 717)\n",
      "('reduced tweet count:', 378)\n",
      "('accuracy fitting to all data:', 0.97089947089947093)\n"
     ]
    }
   ],
   "source": [
    "#X_new,y_new = create_X_and_Y_vectors(filtered_training_tweets)\n",
    "#model = LogisticRegression()\n",
    "model.fit(X_new, y_new)\n",
    "predictions_new = model_new.predict(X_new)\n",
    "print len(y_new)\n",
    "print X_new.shape\n",
    "print('reduced tweet count:', len(y_new))\n",
    "print('accuracy fitting to all data:', accuracy_score(y_new, predictions_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy per 5-fold: [0.34210526315789475, 0.46052631578947367, 0.34210526315789475, 0.37333333333333335, 0.42666666666666669]\n",
      "('average accuracy per fold:', 0.3889473684210526)\n"
     ]
    }
   ],
   "source": [
    "do_cross_validation(X_new,y_new,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy per 10-fold: [0.36842105263157893, 0.44736842105263158, 0.5, 0.55263157894736847, 0.44736842105263158, 0.47368421052631576, 0.39473684210526316, 0.5, 0.56756756756756754, 0.32432432432432434]\n",
      "('average accuracy per fold:', 0.45761024182076826)\n"
     ]
    }
   ],
   "source": [
    "do_cross_validation(X_new,y_new,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def printIncorrectlyLabeledTweets(index, trueLabel, predictedLabel, tweetText):\n",
    "    if trueLabel != predictedLabel:\n",
    "        print('')\n",
    "        print(index)\n",
    "        print ('--- ACTUAL:', (trueLabel), 'PREDICTED:', (predictedLabel), '---')\n",
    "        print (tweetText)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "incorrect_predictions = []\n",
    "# for trainingIndex, testingIndex in KFold(len(y_new), 10):\n",
    "#     model.fit(X_new[trainingIndex], y_new[trainingIndex])\n",
    "#     predictions = model.predict(X_new[testingIndex])\n",
    "#     #accuracyPerFold.append(accuracy_score(y_new[testingIndex], predictions))\n",
    "#     for i, index in enumerate(testingIndex):\n",
    "#         #r = printIncorrectlyLabeledTweets(index, y_new[index], predictions[i], filtered_training_tweets[index]['tweetText'])\n",
    "#         if y_new[index] != predictions[i]:\n",
    "#             incorrect_predictions.append((filtered_training_tweets[index]['tweetText'], y_new[index], predictions[i]))\n",
    "#     print len(incorrect_predictions)\n",
    "#     print '\\n'\n",
    "    \n",
    "# len(incorrect_predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_school_matrix() takes exactly 1 argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-7d7cfee84818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                              ]\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mIIT_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_school_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_iit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: create_school_matrix() takes exactly 1 argument (2 given)"
     ]
    }
   ],
   "source": [
    "unlabeled_iit = ['pickle_outputfile_2015-11-17 22:02:04.txt',\n",
    "                             'pickle_outputfile_2015-11-17 23:47:46.txt',\n",
    "                             'pickle_outputfile_2015-11-18 00:54:18.txt',\n",
    "                             'pickle_outputfile_2015-11-18 02:16:14.txt'\n",
    "                             ]\n",
    "\n",
    "IIT_X = create_school_matrix(unlabeled_iit,X_new.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40039, 24898)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IIT_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of file... exiting\n",
      "end of file... exiting\n",
      "end of file... exiting\n",
      "end of file... exiting\n"
     ]
    }
   ],
   "source": [
    "NU_X = create_school_matrix(['pickle_nu_outputfile_2015-11-26 15:18:08.txt',\n",
    "                            'pickle_nu_outputfile_2015-11-26 16:23:09.txt',\n",
    "                            'pickle_nu_outputfile_2015-11-26 17:28:03.txt',\n",
    "                            'pickle_nu_outputfile_2015-11-26 18:32:47.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 24898 features per sample; expecting 717",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-3cc30595c51f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#unlabeled_tweets = [tweet for tweet in tweetsWithLabels2 if tweet['label'] != '6']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions_IIT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIIT_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reduced tweet count:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_IIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_IIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpercentage_IIT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \"\"\"\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 204\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 24898 features per sample; expecting 717"
     ]
    }
   ],
   "source": [
    "#unlabeled_tweets = [tweet for tweet in tweetsWithLabels2 if tweet['label'] != '6']\n",
    "predictions_IIT = model_new.predict(IIT_X)\n",
    "print('reduced tweet count:', len(predictions_IIT))\n",
    "c1 = Counter(predictions_IIT)\n",
    "percentage_IIT = percentage(c1.values())\n",
    "return zip(c1.keys(),percentage_IIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
